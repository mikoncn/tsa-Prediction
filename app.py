from flask import Flask, render_template, jsonify
import sqlite3
import pandas as pd
import os

app = Flask(__name__)
DB_PATH = 'tsa_data.db'

# è·å–æ•°æ®åº“è¿æ¥çš„åŠ©æ‰‹å‡½æ•°
def get_db_connection():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # å…è®¸é€šè¿‡åˆ—åè®¿é—®ç»“æœ
    return conn

# ä¸»é¡µè·¯ç”±ï¼šè¿”å›ä»ªè¡¨ç›˜ HTML
@app.route('/')
def index():
    return render_template('index.html')

# API: è·å–å†å²æµé‡æ•°æ® (ç”¨äºç»˜åˆ¶ä¸»å›¾è¡¨)
@app.route('/api/data')
def get_data():
    conn = get_db_connection()
    # æŸ¥è¯¢å…¨é‡å®½è¡¨ (åŒ…å«å¤©æ°”å’ŒèŠ‚æ—¥ç‰¹å¾)
    # é™åˆ¶ä¸ºå½“å‰æ—¶é—´ä¹‹å‰çš„æ•°æ®ï¼Œæˆ–è€…å…¨éƒ¨æ•°æ®
    query = """
        SELECT date, throughput, weather_index, is_holiday, holiday_name 
        FROM traffic_full 
        WHERE date <= date('now') 
        ORDER BY date ASC
    """
    try:
        rows = conn.execute(query).fetchall()
    except sqlite3.OperationalError:
        # Fallback if traffic_full doesn't exist yet
        rows = conn.execute('SELECT date, throughput FROM traffic ORDER BY date ASC').fetchall()
        
    conn.close()
    
    data = []
    for row in rows:
        item = {
            'date': row['date'],
            'throughput': row['throughput']
        }
        # Add features if they exist
        if 'weather_index' in row.keys():
            item['weather_index'] = row['weather_index']
            item['is_holiday'] = row['is_holiday']
            item['holiday_name'] = row['holiday_name']
        
        data.append(item)
        
    return jsonify(data)
# API: è·å–é¢„æµ‹ç»“æœå’Œå†å²éªŒè¯æ•°æ®
@app.route('/api/predictions')
def get_predictions():
    result = {}
    
    # 1. åŠ è½½æœªæ¥ 7 å¤©çš„é¢„æµ‹ç»“æœ (ä» train_xgb.py ç”Ÿæˆçš„ CSV)
    try:
        df_forecast = pd.read_csv("xgb_forecast.csv")
        result['forecast'] = df_forecast.to_dict(orient='records')
    except Exception as e:
        result['forecast'] = []
        print(f"Error loading forecast: {e}")

    # 2. åŠ è½½å†å²é¢„æµ‹è®°å½•å¹¶ä¸çœŸå®æµé‡åˆå¹¶ (ç”¨äºæ¨¡å‹å›æµ‹è¡¨æ ¼)
    try:
        print(f"DEBUG: CWD = {os.getcwd()}")
        if os.path.exists("prediction_history.csv"):
            print("DEBUG: History file found.")
            df_hist = pd.read_csv("prediction_history.csv")
            df_actual = pd.read_csv("TSA_Final_Analysis.csv")
            
            print(f"DEBUG: Hist Len={len(df_hist)}, Actual Len={len(df_actual)}")
            
            # Merge Actuals into History
            # df_hist: target_date, predicted_throughput, model_run_date
            # df_actual: date, throughput
            
            # Ensure dates are strings YYYY-MM-DD
            df_hist['target_date'] = pd.to_datetime(df_hist['target_date']).dt.strftime('%Y-%m-%d')
            df_actual['date'] = pd.to_datetime(df_actual['date']).dt.strftime('%Y-%m-%d')
            
            merged = pd.merge(df_hist, df_actual, left_on='target_date', right_on='date', how='inner')
            print(f"DEBUG: Merged Len={len(merged)}")
            
            # Logic: For each target_date, find the prediction made 1 day before (or latest available)
            # Simple approach: Sort by model_run_date desc, drop duplicates on target_date
            merged = merged.sort_values('model_run_date', ascending=False).drop_duplicates('target_date')
            
            # Calculate Error
            merged['difference'] = merged['predicted_throughput'] - merged['throughput']
            merged['error_rate'] = (merged['difference'].abs() / merged['throughput']) * 100
            
            # [FIXES] Drop redundant 'date' from actuals before rename to avoid collision
            if 'date' in merged.columns:
                merged = merged.drop(columns=['date'])

            # Rename for frontend
            merged = merged.rename(columns={
                'target_date': 'date',
                'throughput': 'actual',
                'predicted_throughput': 'predicted'
            })
            
            # Sort by date asc (dashboard.js expects old->new to slice last 15)
            merged = merged.sort_values('date', ascending=True)
            
            count = len(merged)
            print(f"DEBUG: Final Validation Count = {count}")
            
            # [CRITICAL FIX] Handle NaN/Inf for JSON compliance
            # Replace Inf with 0, NaN with 0 (or None)
            merged = merged.fillna(0)
            
            # [å…³é”®é€»è¾‘] åªè¿”å›å‰ç«¯éœ€è¦çš„å­—æ®µ
            result['validation'] = merged[['date', 'actual', 'predicted', 'difference', 'error_rate']].to_dict(orient='records')
        else:
            print("DEBUG: prediction_history.csv NOT found.")
            result['validation'] = []
    except Exception as e:
        result['validation'] = []
        print(f"Error loading validation log: {e}")
        
    return jsonify(result)

# API: ç‚¹å‡»é¡µé¢æŒ‰é’®æ—¶æ‰‹åŠ¨è§¦å‘æ¨¡å‹é‡æ–°è®­ç»ƒå’Œé¢„æµ‹
@app.route('/api/run_prediction', methods=['POST'])
def run_prediction():
    try:
        import subprocess
        import sys
        print("ğŸš€ æ­£åœ¨è§¦å‘æ¨¡å‹è¿è¡Œ (train_xgb.py)...")
        print(f"   Python executable: {sys.executable}")
        print(f"   Working directory: {os.getcwd()}")
        
        # è¿è¡Œå­è¿›ç¨‹æ‰§è¡Œè®­ç»ƒè„šæœ¬
        # æ³¨æ„ï¼šæ­¤å¤„å¤„ç†äº† Windows ç¯å¢ƒä¸‹çš„ GBK ç¼–ç é—®é¢˜
        result = subprocess.run(
            [sys.executable, 'train_xgb.py'], 
            capture_output=True, 
            text=True,
            encoding='utf-8',
            errors='replace',  # è§£ç å¤±è´¥æ—¶æ›¿æ¢å­—ç¬¦è€ŒéæŠ¥é”™
            cwd=os.getcwd(),
            timeout=60  # 60ç§’è¶…æ—¶ä¿æŠ¤
        )
        
        # æ‰“å°å®Œæ•´è¾“å‡ºç”¨äºè°ƒè¯•
        if result.stdout:
            print(f"ğŸ“ STDOUT:\n{result.stdout}")
        if result.stderr:
            print(f"âš ï¸ STDERR:\n{result.stderr}")
        
        if result.returncode == 0:
            print("âœ… Model Run Success")
            # æå–æœ€åå‡ è¡Œè¾“å‡ºä½œä¸ºæ‘˜è¦
            output_lines = result.stdout.strip().split('\n')
            summary = '\n'.join(output_lines[-5:]) if len(output_lines) > 5 else result.stdout
            return jsonify({
                'status': 'success', 
                'message': 'é¢„æµ‹å®Œæˆ!æ•°æ®å·²æ›´æ–°',
                'summary': summary
            })
        else:
            error_msg = result.stderr if result.stderr else result.stdout
            print(f"âŒ Model Run Failed (returncode={result.returncode})")
            return jsonify({
                'status': 'error', 
                'message': f'æ¨¡å‹è¿è¡Œå¤±è´¥: {error_msg}'
            }), 500
            
    except subprocess.TimeoutExpired:
        print(f"âŒ Timeout: æ¨¡å‹è¿è¡Œè¶…è¿‡60ç§’")
        return jsonify({'status': 'error', 'message': 'æ¨¡å‹è¿è¡Œè¶…æ—¶(>60ç§’)'}), 500
    except Exception as e:
        print(f"âŒ Execution Error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'status': 'error', 'message': str(e)}), 500

# API: ä¸€é”®æ›´æ–°æ•°æ®(æŠ“å–TSA+å¤©æ°”+åˆå¹¶)
@app.route('/api/update_data', methods=['POST'])
def update_data():
    try:
        import subprocess
        import sys
        print("ğŸ”„ å¼€å§‹æ•°æ®æ›´æ–°æµç¨‹...")
        
        steps = [
            {'name': 'æŠ“å–æœ€æ–°TSAæ•°æ®', 'cmd': [sys.executable, 'build_tsa_db.py', '--latest'], 'timeout': 30},
            {'name': 'åŒæ­¥å¤©æ°”ç‰¹å¾', 'cmd': [sys.executable, 'get_weather_features.py'], 'timeout': 45},
            {'name': 'åˆå¹¶æ•°æ®åº“', 'cmd': [sys.executable, 'merge_db.py'], 'timeout': 30}
        ]
        
        results = []
        for step in steps:
            print(f"\n[æ­¥éª¤] {step['name']}...")
            result = subprocess.run(
                step['cmd'], capture_output=True, text=True,
                encoding='utf-8', errors='replace',
                cwd=os.getcwd(), timeout=step['timeout']
            )
            
            if result.returncode == 0:
                print(f"âœ… {step['name']} å®Œæˆ")
                output_lines = result.stdout.strip().split('\n')
                summary = '\n'.join(output_lines[-3:]) if len(output_lines) > 3 else result.stdout
                results.append({'step': step['name'], 'status': 'success', 'summary': summary})
            else:
                error_msg = result.stderr if result.stderr else result.stdout
                print(f"âŒ {step['name']} å¤±è´¥: {error_msg}")
                return jsonify({'status': 'error', 'message': f'{step["name"]}å¤±è´¥', 'error': error_msg}), 500
        
        print("\nâœ… æ•°æ®æ›´æ–°æµç¨‹å…¨éƒ¨å®Œæˆ")
        return jsonify({'status': 'success', 'message': 'æ•°æ®æ›´æ–°æˆåŠŸ!', 'results': results})
        
    except subprocess.TimeoutExpired as e:
        print(f"âŒ è¶…æ—¶: {e}")
        return jsonify({'status': 'error', 'message': f'æ“ä½œè¶…æ—¶: {e}'}), 500
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'status': 'error', 'message': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
