from flask import Flask, render_template, jsonify
import sqlite3
import pandas as pd
import os

app = Flask(__name__)

# [NEW] Import config
import sys
# Ensure src can be imported if app.py is run directly
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.config import DB_PATH

# è·å–æ•°æ®åº“è¿æ¥çš„åŠ©æ‰‹å‡½æ•°
def get_db_connection():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # å…è®¸é€šè¿‡åˆ—åè®¿é—®ç»“æœ
    return conn

# ä¸»é¡µè·¯ç”±ï¼šè¿”å›ä»ªè¡¨ç›˜ HTML
@app.route('/')
def index():
    return render_template('index.html')

# API: è·å–å†å²æµé‡æ•°æ® (ç”¨äºç»˜åˆ¶ä¸»å›¾è¡¨)
@app.route('/api/data')
def get_data():
    conn = get_db_connection()
    # æŸ¥è¯¢å…¨é‡å®½è¡¨ (åŒ…å«å¤©æ°”å’ŒèŠ‚æ—¥ç‰¹å¾)
    # é™åˆ¶ä¸ºå½“å‰æ—¶é—´ä¹‹å‰çš„æ•°æ®ï¼Œæˆ–è€…å…¨éƒ¨æ•°æ®
    query = """
        SELECT date, throughput, weather_index, is_holiday, holiday_name 
        FROM traffic_full 
        WHERE date <= date('now') 
        ORDER BY date ASC
    """
    try:
        rows = conn.execute(query).fetchall()
    except sqlite3.OperationalError:
        # Fallback if traffic_full doesn't exist yet
        rows = conn.execute('SELECT date, throughput FROM traffic ORDER BY date ASC').fetchall()
        
    conn.close()
    
    data = []
    for row in rows:
        item = {
            'date': row['date'],
            'throughput': row['throughput']
        }
        # Add features if they exist
        if 'weather_index' in row.keys():
            item['weather_index'] = row['weather_index']
            item['is_holiday'] = row['is_holiday']
            item['holiday_name'] = row['holiday_name']
        
        data.append(item)
        
    return jsonify(data)
# API: è·å–ç”Ÿæ•°æ® (Raw Data) - æ”¯æŒåˆ†é¡µ
@app.route('/api/raw_data')
def get_raw_data():
    try:
        from flask import request
        limit = int(request.args.get('limit', 15))
        offset = int(request.args.get('offset', 0))
        
        conn = get_db_connection()
        
        # åŠ¨æ€æŸ¥è¯¢æ‰€æœ‰å­—æ®µ
        # æˆ‘ä»¬å…ˆæŸ¥ä¸€ä¸‹åˆ—åï¼Œç¡®ä¿å…¨é‡å› å­éƒ½èƒ½è·å–
        # æ ¸å¿ƒå› å­: date, throughput, weather_index, is_holiday, holiday_name, 
        #           flight_volume, days_to_nearest_holiday, is_off_peak_workday, 
        #           is_spring_break, throughput_lag_7
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        check_query = "SELECT name FROM sqlite_master WHERE type='table' AND name='traffic_full'"
        if not conn.execute(check_query).fetchone():
            return jsonify({'error': 'Table traffic_full not ready'}), 404

        # è·å–æ‰€æœ‰åˆ—å
        cursor = conn.execute("PRAGMA table_info(traffic_full)")
        columns = [row['name'] for row in cursor.fetchall()]
        
        # æ„å»ºæŸ¥è¯¢
        col_str = ", ".join(columns)
        # [FIX] User requested to limit future data to T+3 days to avoid empty rows
        query = f"SELECT {col_str} FROM traffic_full WHERE date <= date('now', '+3 days') ORDER BY date DESC LIMIT ? OFFSET ?"
        
        rows = conn.execute(query, (limit, offset)).fetchall()
        conn.close()
        
        data = []
        for row in rows:
            # å°† sqlite.Row è½¬ä¸ºæ™®é€š dict
            item = dict(row)
            data.append(item)
            
        return jsonify({
            'status': 'success',
            'data': data,
            'pagination': {'limit': limit, 'offset': offset}
        })
        
    except Exception as e:
        print(f"Error in get_raw_data: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

# API: è·å–é¢„æµ‹ç»“æœå’Œå†å²éªŒè¯æ•°æ®
@app.route('/api/predictions')
def get_predictions():
    result = {}
    
    try:
        conn = sqlite3.connect('tsa_data.db')
        conn.row_factory = sqlite3.Row
        
        from datetime import datetime
        now_str = datetime.now().strftime('%Y-%m-%d')
        
        # 1. åŠ è½½æœªæ¥é¢„æµ‹ (Forecast) - From SQLite 'prediction_history'
        # Logic: Get predictions for Date >= Today
        # Logic: Get predictions for Date >= Today
        query_forecast = """
            SELECT target_date, predicted_throughput, model_run_date, 
                   weather_index, is_holiday, flight_volume, holiday_name 
            FROM prediction_history 
            WHERE target_date >= ?
        """
        df_preds = pd.read_sql(query_forecast, conn, params=(now_str,))
        
        if not df_preds.empty:
            # Dedupe: keep latest model_run_date for each target_date
            df_preds['target_date'] = pd.to_datetime(df_preds['target_date']).dt.strftime('%Y-%m-%d')
            # Sort by run_date DESC, keep first
            df_forecast = df_preds.sort_values('model_run_date', ascending=False).drop_duplicates('target_date')
            # Sort by date ASC for chart
            df_forecast = df_forecast.sort_values('target_date')
            # Fill NaNs for display
            df_forecast[['weather_index', 'is_holiday', 'flight_volume']] = df_forecast[['weather_index', 'is_holiday', 'flight_volume']].fillna(0)
            
            result['forecast'] = df_forecast[['target_date', 'predicted_throughput', 'weather_index', 'is_holiday', 'flight_volume', 'holiday_name']].rename(columns={
                'target_date': 'ds',
                'predicted_throughput': 'predicted_throughput'
            }).to_dict(orient='records')
        else:
            result['forecast'] = []

        # 2. åŠ è½½å†å²éªŒè¯ (Validation) - From SQLite 'prediction_history' & 'traffic_full'
        # Query History (Past predictions)
        query_hist = """
            SELECT target_date, predicted_throughput, model_run_date, 
                   weather_index, is_holiday, flight_volume
            FROM prediction_history 
            WHERE target_date < ?
        """
        df_hist = pd.read_sql(query_hist, conn, params=(now_str,))
        
        # [FIX] Generate 'History' (Orange Line) separately from Validation
        # History should show ALL past predictions, even if we don't have actuals yet.
        if not df_hist.empty:
            # 1. Standardize formatting
            df_hist['target_date'] = pd.to_datetime(df_hist['target_date']).dt.strftime('%Y-%m-%d')
            # 2. Keep latest prediction per date
            df_hist_clean = df_hist.sort_values('model_run_date', ascending=False).drop_duplicates('target_date')
            # 3. Sort for chart
            df_hist_clean = df_hist_clean.sort_values('target_date')
            # Fill NaNs
            df_hist_clean[['weather_index', 'is_holiday', 'flight_volume']] = df_hist_clean[['weather_index', 'is_holiday', 'flight_volume']].fillna(0)
            
            result['history'] = df_hist_clean[['target_date', 'predicted_throughput', 'weather_index', 'is_holiday', 'flight_volume']].rename(columns={
                'target_date': 'date',
                'predicted_throughput': 'predicted'
            }).to_dict(orient='records')
        else:
            result['history'] = []
            
        # Query Actuals (From traffic_full)
        query_actual = "SELECT date, throughput FROM traffic_full WHERE throughput IS NOT NULL"
        df_actual = pd.read_sql(query_actual, conn)
        
        conn.close()
        
        # Validation Table (Only where we have BOTH Prediction AND Actuals)
        if not df_hist.empty and not df_actual.empty:
            # Standardization
            # df_hist['target_date'] is already standardized above
            df_actual['date'] = pd.to_datetime(df_actual['date']).dt.strftime('%Y-%m-%d')
            
            # Merge
            merged = pd.merge(df_hist, df_actual, left_on='target_date', right_on='date', how='inner')
            
            # Keep latest prediction per target date (using target_date for dedupe logic works, or date)
            merged = merged.sort_values('model_run_date', ascending=False).drop_duplicates('target_date')
            
            # Calculate Error
            merged['difference'] = merged['predicted_throughput'] - merged['throughput']
            merged['error_rate'] = (merged['difference'].abs() / merged['throughput']) * 100
            
            # Formatting
            # We already have 'date' from df_actual. We don't need to rename target_date to date.
            # But we might need to ensure target_date is dropped or just ignored.
            merged = merged.rename(columns={
                'throughput': 'actual',
                'predicted_throughput': 'predicted'
            })
            
            # Select columns explicitly
            merged = merged[['date', 'actual', 'predicted', 'difference', 'error_rate']]
            
            merged = merged.sort_values('date', ascending=True)
            merged = merged.fillna(0)
            
            result['validation'] = merged[['date', 'actual', 'predicted', 'difference', 'error_rate']].to_dict(orient='records')
        else:
            result['validation'] = []
            
    except Exception as e:
        print(f"Error in get_predictions (DB Mode): {e}")
        import traceback
        traceback.print_exc()
        result['forecast'] = []
        result['validation'] = []
        result['history'] = []
        
    return jsonify(result)

# API: ç‚¹å‡»é¡µé¢æŒ‰é’®æ—¶æ‰‹åŠ¨è§¦å‘æ¨¡å‹é‡æ–°è®­ç»ƒå’Œé¢„æµ‹
@app.route('/api/run_prediction', methods=['POST'])
def run_prediction():
    try:
        import subprocess
        import sys
        print("ğŸš€ æ­£åœ¨è§¦å‘æ¨¡å‹è¿è¡Œ (train_xgb.py)...")
        print(f"   Python executable: {sys.executable}")
        print(f"   Working directory: {os.getcwd()}")
        
        # è¿è¡Œå­è¿›ç¨‹æ‰§è¡Œè®­ç»ƒè„šæœ¬
        # æ³¨æ„ï¼šæ­¤å¤„å¤„ç†äº† Windows ç¯å¢ƒä¸‹çš„ GBK ç¼–ç é—®é¢˜
        result = subprocess.run(
            [sys.executable, '-m', 'src.models.train_xgb'], 
            capture_output=True, 
            text=True,
            encoding='utf-8',
            errors='replace',  # è§£ç å¤±è´¥æ—¶æ›¿æ¢å­—ç¬¦è€ŒéæŠ¥é”™
            cwd=os.getcwd(),
            timeout=60  # 60ç§’è¶…æ—¶ä¿æŠ¤
        )
        
        # æ‰“å°å®Œæ•´è¾“å‡ºç”¨äºè°ƒè¯•
        if result.stdout:
            print(f"ğŸ“ STDOUT:\n{result.stdout}")
        if result.stderr:
            print(f"âš ï¸ STDERR:\n{result.stderr}")
        
        if result.returncode == 0:
            print("âœ… Model Run Success")
            # æå–æœ€åå‡ è¡Œè¾“å‡ºä½œä¸ºæ‘˜è¦
            output_lines = result.stdout.strip().split('\n')
            summary = '\n'.join(output_lines[-5:]) if len(output_lines) > 5 else result.stdout
            return jsonify({
                'status': 'success', 
                'message': 'é¢„æµ‹å®Œæˆ!æ•°æ®å·²æ›´æ–°',
                'summary': summary
            })
        else:
            error_msg = result.stderr if result.stderr else result.stdout
            print(f"âŒ Model Run Failed (returncode={result.returncode})")
            return jsonify({
                'status': 'error', 
                'message': f'æ¨¡å‹è¿è¡Œå¤±è´¥: {error_msg}'
            }), 500
            
    except subprocess.TimeoutExpired:
        print(f"âŒ Timeout: æ¨¡å‹è¿è¡Œè¶…è¿‡60ç§’")
        return jsonify({'status': 'error', 'message': 'æ¨¡å‹è¿è¡Œè¶…æ—¶(>60ç§’)'}), 500
    except Exception as e:
        print(f"âŒ Execution Error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'status': 'error', 'message': str(e)}), 500

# API: ä¸€é”®æ›´æ–°æ•°æ®(æŠ“å–TSA+å¤©æ°”+åˆå¹¶)
@app.route('/api/update_data', methods=['POST'])
def update_data():
    try:
        import subprocess
        import sys
        print("ğŸ”„ å¼€å§‹æ•°æ®æ›´æ–°æµç¨‹...")
        
        steps = [
            {'name': 'æŠ“å–æœ€æ–°TSAæ•°æ®', 'cmd': [sys.executable, '-m', 'src.etl.build_tsa_db', '--latest'], 'timeout': 30},
            {'name': 'åŒæ­¥å¤©æ°”ç‰¹å¾', 'cmd': [sys.executable, '-m', 'src.etl.get_weather_features'], 'timeout': 45},
            {'name': 'åˆå¹¶æ•°æ®åº“', 'cmd': [sys.executable, '-m', 'src.etl.merge_db'], 'timeout': 30},
            {'name': 'å…¨é‡æ¨¡å‹é‡è®­(Persistence)', 'cmd': [sys.executable, '-m', 'src.models.train_xgb'], 'timeout': 120}
        ]
        
        results = []
        for step in steps:
            print(f"\n[æ­¥éª¤] {step['name']}...")
            result = subprocess.run(
                step['cmd'], capture_output=True, text=True,
                encoding='utf-8', errors='replace',
                cwd=os.getcwd(), timeout=step['timeout']
            )
            
            if result.returncode == 0:
                print(f"âœ… {step['name']} å®Œæˆ")
                output_lines = result.stdout.strip().split('\n')
                summary = '\n'.join(output_lines[-3:]) if len(output_lines) > 3 else result.stdout
                results.append({'step': step['name'], 'status': 'success', 'summary': summary})
            else:
                error_msg = result.stderr if result.stderr else result.stdout
                print(f"âŒ {step['name']} å¤±è´¥: {error_msg}")
                return jsonify({'status': 'error', 'message': f'{step["name"]}å¤±è´¥', 'error': error_msg}), 500
        
        print("\nâœ… æ•°æ®æ›´æ–°æµç¨‹å…¨éƒ¨å®Œæˆ")
        return jsonify({'status': 'success', 'message': 'æ•°æ®æ›´æ–°æˆåŠŸ!', 'results': results})
        
    except subprocess.TimeoutExpired as e:
        print(f"âŒ è¶…æ—¶: {e}")
        return jsonify({'status': 'error', 'message': f'æ“ä½œè¶…æ—¶: {e}'}), 500
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'status': 'error', 'message': str(e)}), 500

# API: ç‹™å‡»æ¨¡å‹ (T+0 Nowcasting)
@app.route('/api/predict_sniper', methods=['POST'])
def predict_sniper():
    try:
        import subprocess
        import sys
        import json
        
        # Determine target date? For now default to script default (Today/Tomorrow)
        # Or accept from JSON body if needed
        
        print("ğŸ¯ å¯åŠ¨ç‹™å‡»æ¨¡å‹ (Sniper Mode)...")
        
        # Run script
        result = subprocess.run(
            [sys.executable, '-m', 'src.models.predict_sniper'],
            capture_output=True,
            text=True,
            encoding='utf-8', 
            errors='replace',
            cwd=os.getcwd(),
            timeout=30 # Fast timeout
        )
        
        if result.returncode == 0:
            # Parse JSON from stdout
            try:
                # Script might print other things, find the JSON line
                lines = result.stdout.strip().split('\n')
                # Assume last line is JSON
                json_str = lines[-1]
                data = json.loads(json_str)
                
                # [FIX] Check for internal script error
                if "error" in data:
                     print(f"âŒ Sniper Internal Error: {data['error']}")
                     return jsonify({'status': 'error', 'message': data['error']}), 500
                     
                print(f"âœ… Sniper Hit: {data}")
                return jsonify({'status': 'success', 'data': data})
            except Exception as parse_err:
                print(f"âš ï¸ JSON Parse Error: {parse_err}. Stdout: {result.stdout}")
                return jsonify({'status': 'error', 'message': 'æ— æ³•è§£ææ¨¡å‹è¾“å‡º', 'raw': result.stdout}), 500
        else:
            print(f"âŒ Sniper Missed: {result.stderr}")
            return jsonify({'status': 'error', 'message': 'æ¨¡å‹è¿è¡Œå¤±è´¥', 'error': result.stderr}), 500
            
    except Exception as e:
        print(f"âŒ Sniper Error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/run_challenger', methods=['POST'])
def run_challenger():
    """è§¦å‘ FLAML æ·±åº¦åˆ†æ (Challenger Model)"""
    try:
        import subprocess
        import sys
        import json
        
        print("ğŸŸ£ å¯åŠ¨ FLAML æŒ‘æˆ˜è€…è®­ç»ƒä»»åŠ¡...")
        
        # è¿è¡Œè®­ç»ƒè„šæœ¬
        result = subprocess.run(
            [sys.executable, '-m', 'src.models.train_challenger'],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',
            timeout=600 # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode != 0:
            return jsonify({
                'status': 'error', 
                'message': f"Training failed: {result.stderr}"
            }), 500
            
        # è¯»å–ç”Ÿæˆçš„æ‘˜è¦
        if os.path.exists("challenger_summary.json"):
            with open("challenger_summary.json", 'r') as f:
                summary = json.load(f)
            return jsonify({
                'status': 'success',
                'data': summary
            })
        else:
             return jsonify({
                'status': 'error', 
                'message': "Model trained but no summary file found."
            }), 500
            
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/market_sentiment')
def get_market_sentiment():
    """è·å– Polymarket å¸‚åœºæƒ…ç»ª (å« 6H æ¶¨è·Œå¹…)"""
    try:
        conn = get_db_connection()
        
        # 1. è·å–æ¯ä¸ª (date, outcome) çš„æœ€æ–°ä»·æ ¼
        # ä½¿ç”¨çª—å£å‡½æ•°æˆ– Group By Max ID (SQLiteç®€å•å¤„ç†)
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦é’ˆå¯¹æ¯ä¸ª target_date + outcome_label åˆ†ç»„
        
        query_latest = """
            SELECT target_date, outcome_label, price as current_price, fetched_at, market_slug
            FROM market_sentiment_snapshots 
            WHERE id IN (
                SELECT MAX(id) 
                FROM market_sentiment_snapshots 
                GROUP BY target_date, outcome_label
            )
            ORDER BY target_date ASC, outcome_label ASC
        """
        
        rows_latest = conn.execute(query_latest).fetchall()
        
        # 2. è·å– ~6å°æ—¶å‰çš„ä»·æ ¼ (æˆ–æœ€æ¥è¿‘çš„æ—§æ•°æ®)
        # ç®€å•ç­–ç•¥ï¼šæŸ¥æ‰¾ fetched_at <= now - 6h çš„æœ€æ–°ä¸€æ¡
        # ä½†è¿™ç§å¯¹äºæ‰¹é‡æŸ¥è¯¢å¾ˆæ…¢ã€‚
        # ä¼˜åŒ–ç­–ç•¥ï¼šLoad full recent history in memory (volume is low enough) OR single complex query.
        # é‰´äºæ•°æ®é‡æ¯å¤©ä»…å‡ ç™¾æ¡ï¼Œè½½å…¥å†…å­˜å¤„ç†æœ€å¿«ã€‚
        
        # Let's use Python to compute diffs from raw rows for simplicity and robustness
        query_all_recent = """
            SELECT target_date, outcome_label, price, fetched_at
            FROM market_sentiment_snapshots
            WHERE fetched_at >= datetime('now', '-24 hours')
            ORDER BY fetched_at ASC
        """
        all_rows = conn.execute(query_all_recent).fetchall()
        conn.close()
        
        from datetime import datetime
        import pandas as pd
        
        # Group by key
        history_map = {} # Key: "date|outcome" -> List of (dt, price)
        
        for r in all_rows:
            key = f"{r['target_date']}|{r['outcome_label']}"
            fetched_dt = datetime.strptime(r['fetched_at'], '%Y-%m-%d %H:%M:%S')
            if key not in history_map:
                history_map[key] = []
            history_map[key].append((fetched_dt, r['price']))
            
        results = []
        
        # Process latest rows
        now = datetime.utcnow() # SQLite usually UTC
        # If SQLite fetched_at is local, might need adjustment. defaulted to CURRENT_TIMESTAMP (UTC).
        
        # Re-iterate latest rows from SQL (or compute from history map latest)
        # Using SQL latest is safer
        for r in rows_latest:
            key = f"{r['target_date']}|{r['outcome_label']}"
            curr_price = r['current_price']
            slug = r['market_slug']
            
            # Find 6h ago price
            # Ideal: Price at (Now - 6h)
            # Logic: Find closest snapshot that is older than 5.5h? Or just finding the one closest to 6h mark?
            # Let's try to find a data point between 5h and 7h ago.
            # If not found, fallback to oldest available within 24h?
            
            change_6h = 0.0
            
            if key in history_map:
                points = history_map[key]
                # Points are sorted ASC by time
                # We want point closest to (latest_time - 6h)
                
                # Assume latest fetch was just now-ish
                latest_ts = points[-1][0]
                target_ts = latest_ts - pd.Timedelta(hours=6)
                
                closest_price = None
                min_diff_seconds = 999999
                
                for (ts, p) in points:
                    # check difference
                    diff = abs((ts - target_ts).total_seconds())
                    # We only care if the point is ACTUALLY in the past relative to latest
                    # and roughly around the 6h mark (e.g., within 3h to 9h window?)
                    # Simplification: Just find the record closest to target_ts
                    
                    if diff < min_diff_seconds:
                        min_diff_seconds = diff
                        closest_price = p
                
                # Calculate change
                if closest_price is not None:
                     change_6h = curr_price - closest_price
                     
            results.append({
                'target_date': r['target_date'],
                'market_slug': slug,
                'outcome': r['outcome_label'],
                'price': curr_price,
                'change_6h': round(change_6h, 3),
                'fetched_at': r['fetched_at']
            })
            
        # Group by Date for frontend convenience
        grouped = {}
        for item in results:
            d = item['target_date']
            if d not in grouped: grouped[d] = []
            grouped[d].append(item)
            
        return jsonify(grouped)
        
    except Exception as e:
        print(f"Error in market_sentiment: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, port=5001)
